{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "# **Семинар 1: Классификация обращений граждан**\n",
        "## Содержание занятия:\n",
        "\n",
        "\n",
        "### Тема 1. Предварительная обработка текстовых данных (Data preprocessing)\n",
        "Рассматриваемые вопросы:\n",
        "\n",
        "*   Токенизация\n",
        "*   Удаление нерелевантных символов\n",
        "*   Удаление стоп-слов\n",
        "*   Приведение слова к нормальной форме\n",
        "\n",
        "### Тема 2. Модель классификации текстов\n",
        "Рассматриваемые вопросы:\n",
        "\n",
        "*   Exploratory Data Analysis\n",
        "*   Предварительная обработка данных\n",
        "*   Формирование векторов/массивов признаков (feature vectors) для обучения модели\n",
        "*   Кодирование категориальных целевых перемменных (Labelling)\n",
        "*   Построение простейшей нейронной сети для классификации в Tensorflow.Keras\n",
        "*   Оценка качества обучения  \n",
        "\n"
      ],
      "metadata": {
        "id": "xomt0LDA2Jln"
      },
      "id": "xomt0LDA2Jln"
    },
    {
      "cell_type": "markdown",
      "source": [
        "Начнём с импорта необходимых библиотек:\n",
        "\n",
        "*   **pandas** — библиотека, которая применяется для обработки и анализа табличных данных. В этой библиотеке используется numpy для удобного хранения данных и вычислений.\n",
        "*   **re**  — это встроенный модуль для работы с регулярными выражениями (regular expressions). С её помощью можно выполнять поиск, замену, разбор строк по шаблонам и многое другое.\n",
        "*   **pymorphy3** — это морфологический анализатор для русского языка (а также для других славянских языков), написанный на Python. Он позволяет определять грамматические характеристики слова, лемматизировать (приводить к начальной форме), и делать разбор слов.\n",
        "*   **NLTK** (Natural Language Processing Toolkit) -  пакет библиотек и программ для символьной и статистической обработки естественного языка.\n",
        "*   **matplotlib.pyplot** - библиотека для построения графиков. Типы графиков, которые можно построить: https://matplotlib.org/stable/plot_types/index.html.\n",
        "*   **tensorflow.keras** - для работы с нейросетями. Класс **Tokenizer** из модуля tensorflow.keras.preprocessing.text — это утилита для предобработки текста перед подачей в нейросеть. Он используется для токенизации — превращения текста в последовательности чисел. (Сейчас уже рекомендуется использовать tf.keras.layers.TextVectorization или tensorflow_text)\n",
        "*   **scikit-learn** (обычно импортируется как sklearn) — одна из самых популярных библиотек для машинного обучения и анализа данных\n",
        "\n"
      ],
      "metadata": {
        "id": "rmCRoMEIEWa1"
      },
      "id": "rmCRoMEIEWa1"
    },
    {
      "cell_type": "code",
      "source": [
        "# Загружаем в Google Colab внешнюю библиотеку\n",
        "!pip install pymorphy3\n",
        "# pip install pymorphy3-dicts-ru"
      ],
      "metadata": {
        "id": "aaH4bNxYyzDR"
      },
      "id": "aaH4bNxYyzDR",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ab631bd4",
      "metadata": {
        "id": "ab631bd4"
      },
      "outputs": [],
      "source": [
        "# Импорт необходимых библиотек\n",
        "# Успешное выполнение этой ячейки кода подтверждает правильную настройку среды разработки\n",
        "\n",
        "import numpy as np # Для работы с массивами\n",
        "import pandas as pd\n",
        "import re\n",
        "import pymorphy3\n",
        "import nltk\n",
        "import matplotlib.pyplot as plt # Для вывода графиков\n",
        "%matplotlib inline\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer # Метод для работы с текстами и преобразования их в последовательности токенов\n",
        "\n",
        "nltk.download('punkt_tab')\n",
        "nltk.download('stopwords')\n",
        "\n",
        "from tensorflow.keras import utils # Для работы с категориальными переменными\n",
        "from keras.utils import plot_model\n",
        "from tensorflow.keras.models import Sequential # Полносвязная модель\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.layers import Dense, Dropout, SpatialDropout1D, BatchNormalization, Embedding, Flatten, Activation # Слои для нейросети\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences # Метод для работы с последовательностями\n",
        "\n",
        "from sklearn.preprocessing import LabelEncoder # Метод кодирования категориальных целевых перемменных\n",
        "from sklearn.model_selection import train_test_split # Для разделения выборки на тестовую и обучающую"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Если нужно, подключаемся к Google Drive, чтобы получить доступ к данным на диске через pd.read_csv('/content/drive/MyDrive/.../Lesson_1_user_requests.csv')\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "Sz9OpiAUy97r"
      },
      "id": "Sz9OpiAUy97r",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Читаем данные из файла и смотрим первые несколько строк таблицы с обращениями граждан\n",
        "data = pd.read_csv('/content/drive/MyDrive/RUT_NLP_MLOps/Неделя_1/Семинар_1/Lesson_1_user_requests.csv')\n",
        "data.head()"
      ],
      "metadata": {
        "id": "Dylg2SRAzX6o"
      },
      "id": "Dylg2SRAzX6o",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "7d195617",
      "metadata": {
        "id": "7d195617"
      },
      "source": [
        "# Тема 1. Предварительная обработка текстовых данных (Data preprocessing)\n",
        "\n",
        "В классическом случае предобработка текстовых данных начинается с определения языка, на котором написан текст. Для этого используются либо код языка, который часто идёт вместе с сообщением при парсинге данных из соцсетей, либо специальные библиотеки. Нам заранее известно, что сегодня мы работаем только с данными на русском языке, поэтому мы пропускаем этот шаг.\n",
        "\n",
        "## Токенизация\n",
        "\n",
        "Токенизация — это процесс разбиения текста на более мелкие единицы (токены), которые удобнее анализировать и обрабатывать. В нашем случае токенами будут отдельные слова. Однако, в зависимости от задачи, токенами могут быть отдельные символы, сочетания из нескольких букв, словосочетания, предложения, и даже абзацы.\n",
        "\n",
        "Для примера мы используем токенизатор одной из самых популярных библиотек обработки естесственного языка - **NLTK** (Natural Language Processing Toolkit). С ее помощью можно анализировать тексты на русском, английском, немецком и других языках.\n",
        "\n",
        "## Удаление нерелевантных символов\n",
        "\n",
        "Удаление различных знаков препинания и специальных символов. В зависимости от задачи необходимость в удалении символов может отсутствовать.\n",
        "\n",
        "## Удаление стоп-слов\n",
        "\n",
        "Стоп-слова — это часто встречающиеся служебные слова (например: и, в, на, это, что, как), которые обычно не несут значимой смысловой нагрузки для анализа текста (предлоги, союзы, местоимения, междометия и тд., точный перечень зависит от конкретной решаемой задачи).\n",
        "\n",
        "Ниже приведён пример использования NLTK для токенизации, удаления нерелевантных символов, и удаления стоп-слов.\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Токенизация с помощью nltk\n",
        "sample_text = 'Перед тем как читать дальше, загадай парня (девушку). Загадал(а)? Читай дальше.'\n",
        "tokenized = nltk.tokenize.word_tokenize(sample_text)\n",
        "print(tokenized)"
      ],
      "metadata": {
        "id": "jz-0HAP9CP0v"
      },
      "id": "jz-0HAP9CP0v",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Удаление нерелевантных символов\n",
        "words = [word.lower() for word in tokenized if word.isalpha()]\n",
        "print(words)"
      ],
      "metadata": {
        "id": "asLWmT5msZIw"
      },
      "id": "asLWmT5msZIw",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Пример использования регулярных выражений для удаления нерелевантных символов.\n",
        "# В данном случае всё, кроме последовательностей букв, цифр, подчёркиваний, и пробелов заменяется пустыми сроками.\n",
        "sample_text = 'Перед тем как читать дальше, загадай парня (девушку). Загадал(а)? Читай дальше.'\n",
        "print(re.sub(r'[^\\w\\s]', '', sample_text))"
      ],
      "metadata": {
        "id": "A-C29mCcAv7J"
      },
      "id": "A-C29mCcAv7J",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Удаление стоп-слов\n",
        "stopwords_ru = nltk.corpus.stopwords.words(\"russian\")\n",
        "words_without_stop = [word for word in words if word not in stopwords_ru]\n",
        "print(words_without_stop)"
      ],
      "metadata": {
        "id": "eqP0ogAVHxWZ"
      },
      "id": "eqP0ogAVHxWZ",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Использование Tokenizer из tensorflow.keras.preprocessing.text\n",
        "\n",
        "Часто несколько шагов из предварительной обработки текстовых данных объединяют в один метод. Рассмотрим как пример метод fit_on_texts() класса Tokenizer для построения словаря (word index) на основе корпуса текстов.\n",
        "\n",
        "Параметры:\n",
        "*   num_words=maxWordsCount - определяем максимальное количество слов/индексов, учитываемое при преобразовании текста в последовательность индексов слов (в полученной последовательности используются только maxWordsCount-1 самых частых слов).\n",
        "*   filters='!\"#$%&()*+,-./:;<=>?@[\\\\]^_`{|}~\\t\\n' - избавляемся от ненужных символов\n",
        "*   lower=True - приводим слова к нижнему регистру\n",
        "*  oov_token='unknown' - токен для слов, которых нет в словаре\n",
        "*   split=' ' - разделяем слова по пробелу\n",
        "*   char_level=False - токенизируем по словам (Если будет True - каждый символ будет рассматриваться как отдельный токен)\n",
        "\n",
        "Происходит следующее:\n",
        "\n",
        "*   Разбиение текста на токены (по умолчанию — по пробелам, с удалением символов из filters).\n",
        "\n",
        "*   Подсчёт частоты встречаемости слов.\n",
        "\n",
        "*   Формирование словаря: каждому уникальному слову присваивается индекс (начиная с 1).\n",
        "\n",
        "*   Индексы назначаются в порядке убывания частоты (самое частое слово получит индекс 1, следующее — 2 и т. д.).\n",
        "\n",
        "*   0 обычно зарезервирован под padding."
      ],
      "metadata": {
        "id": "EvzABhz75j73"
      },
      "id": "EvzABhz75j73"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "587e4a36",
      "metadata": {
        "id": "587e4a36"
      },
      "outputs": [],
      "source": [
        "sample_text = ['Перед тем как читать дальше, загадай парня (девушку). Загадал(а)? Читай дальше.']\n",
        "maxWordsCount = 5\n",
        "\n",
        "tokenizer = Tokenizer(num_words=maxWordsCount, filters='!\"#$%&()*+,-–—./…:;<=>?@[\\\\]^_`{|}~«»\\t\\n\\xa0\\ufeff', lower=True, split=' ', oov_token='unknown', char_level=False)\n",
        "\n",
        "tokenizer.fit_on_texts(sample_text) # \"Скармливаем\" наши тексты, т.е. даём в обработку методу, который соберет словарь частотности\n",
        "print(\"Количество слов в тексте: \", len(sample_text[0].split()))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a2df456e",
      "metadata": {
        "id": "a2df456e"
      },
      "outputs": [],
      "source": [
        "# Полученный словарь для необработанного текста\n",
        "tokenizer.word_index"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Приведение слова к нормальной форме (лемматизация и стемминг)\n",
        "\n",
        "Обычно тексты содержат разные грамматические формы одного и того же слова, а также могут встречаться однокоренные слова, близкие по значению. Нормализацию используют для приведения всех встречающиеся форм слова к одной, нормальной форме.\n",
        "\n",
        "Способы нормализации слов:\n",
        "\n",
        "*   Лемматизация (более точная)\n",
        "Приводит слово к словарной (начальной) форме:\n",
        "\n",
        "детей → ребенок, делаешь → делать.\n",
        "\n",
        "Инструменты: pymorphy3, spaCy, Stanza.\n",
        "\n",
        "*   Стемминг (более грубый)\n",
        "Отрезает окончания, иногда искажая слова:\n",
        "\n",
        "машины, машиной → машин\n",
        "\n",
        "Инструменты: nltk.stem.SnowballStemmer (есть русский).\n",
        "\n",
        "Ниже приведён пример лемматизации с помощью pymorphy3"
      ],
      "metadata": {
        "id": "UhLdzWgb6GPR"
      },
      "id": "UhLdzWgb6GPR"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "15bb4ef1",
      "metadata": {
        "code_folding": [],
        "id": "15bb4ef1"
      },
      "outputs": [],
      "source": [
        "sample_text = 'Перед тем как читать дальше, загадай парня (девушку). Загадал(а)? Читай дальше.'\n",
        "\n",
        "# создаём морфологический анализатор\n",
        "morph = pymorphy3.MorphAnalyzer()\n",
        "\n",
        "# удаляем пунктуацию\n",
        "clean_text = re.sub(r\"[^\\w\\s]\", \"\", sample_text)\n",
        "\n",
        "# разбиваем на слова\n",
        "words = clean_text.split()\n",
        "\n",
        "# приводим к леммам\n",
        "lemmas = [morph.parse(word)[0].normal_form for word in words]\n",
        "\n",
        "print(\"Исходный текст:\", sample_text)\n",
        "print(\"Лемматизированный текст в формате списка: \", lemmas)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Построим новый словарь и посмотрим, что изменилось\n",
        "tokenizer = Tokenizer(num_words=maxWordsCount, filters='!\"#$%&()*+,-–—./…:;<=>?@[\\\\]^_`{|}~«»\\t\\n\\xa0\\ufeff', lower=True, split=' ', oov_token='unknown', char_level=False)\n",
        "\n",
        "tokenizer.fit_on_texts(lemmas)\n",
        "\n",
        "tokenizer.word_index"
      ],
      "metadata": {
        "id": "4wSdDB22F7f6"
      },
      "id": "4wSdDB22F7f6",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Мы видим, что количество уникальных слов в словаре уменьшилось за счёт повторяющихся нормализованных слов.\n"
      ],
      "metadata": {
        "id": "3qJP6jZpGPRg"
      },
      "id": "3qJP6jZpGPRg"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "62396c3b",
      "metadata": {
        "id": "62396c3b"
      },
      "outputs": [],
      "source": [
        "# Количество слов в документе\n",
        "tokenizer.word_counts"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Тема 2. Модель классификации текстов"
      ],
      "metadata": {
        "id": "d-T3JVzH79MS"
      },
      "id": "d-T3JVzH79MS"
    },
    {
      "cell_type": "markdown",
      "id": "850284bb",
      "metadata": {
        "id": "850284bb"
      },
      "source": [
        "# Обращения граждан\n",
        "\n",
        "Рассмотрим пример работы с текстом на реальных данных: обращения граждан с различными вопросами и проблемами, которые нужно классифицировать по темам."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "de467c57",
      "metadata": {
        "id": "de467c57"
      },
      "outputs": [],
      "source": [
        "# data = pd.read_csv('Lesson_1_user_requests.csv') #загружаем данные в dataframe, если ещё не сделали это ранее\n",
        "data.head(10)    #посмотрим на содержимое"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5e6a06f6",
      "metadata": {
        "id": "5e6a06f6"
      },
      "outputs": [],
      "source": [
        "# Нас интересуют только значения \"process_texts\" - тексты самих обращений, и \"sphera\" - тема, к которой относится письмо\n",
        "df_text = data.copy(deep=True)\n",
        "df_text = df_text[['process_texts','sphera']]\n",
        "df_text.head(10)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0ee1fa14",
      "metadata": {
        "id": "0ee1fa14"
      },
      "source": [
        "## Exploratory Data Analysis (EDA) - Первичный/разведочный анализ данных\n",
        "\n",
        "Цели:\n",
        "\n",
        "\n",
        "*   Понимание структуры и характеристик набора данных\n",
        "*   Выявление аномалий и выбросов\n",
        "*   Идентификация связей и корреляций между переменными (в нашем случае не нужно)\n",
        "*   Подготовка данных для дальнейших этапов анализа\n",
        "\n",
        "Начнём с оценки общего колчества обращений, пропущенных значений в таблице, и повторов."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d4439b0d",
      "metadata": {
        "id": "d4439b0d"
      },
      "outputs": [],
      "source": [
        "print(\"Всего строк в таблице: \", df_text.shape[0])\n",
        "\n",
        "# Поиск пропущенных значений\n",
        "\n",
        "df_text.isna().sum()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e004983d",
      "metadata": {
        "id": "e004983d"
      },
      "outputs": [],
      "source": [
        "# Удаляем строки с пропущенными значениями\n",
        "\n",
        "df_text = df_text.dropna()\n",
        "print(\"Всего строк в таблице без пропущенных значений: \", df_text.shape[0])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6aa5f252",
      "metadata": {
        "id": "6aa5f252"
      },
      "outputs": [],
      "source": [
        "# Ищем повторяющиеся обращения (полные дубликаты)\n",
        "\n",
        "duplicated_mask = df_text.duplicated(subset = 'process_texts', keep = False)\n",
        "print(f'Всего {np.sum(duplicated_mask)} повторяющихся обращений в наборе данных.\\n')\n",
        "df_text[duplicated_mask].sort_values(by = 'process_texts').head(11)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df_text = df_text.drop_duplicates(subset = 'process_texts', keep = 'first')\n",
        "print(f'Таблца без повторяющихся обращений содержит {df_text.shape[0]} строк.')"
      ],
      "metadata": {
        "id": "xk7NVu3xOGWY"
      },
      "id": "xk7NVu3xOGWY",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Предварительная обработка данных"
      ],
      "metadata": {
        "id": "hdgTHcdj8nES"
      },
      "id": "hdgTHcdj8nES"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c085a8ae",
      "metadata": {
        "id": "c085a8ae"
      },
      "outputs": [],
      "source": [
        "# Удаляем рабочие символы и знаки припенания\n",
        "\n",
        "df_text['process_texts'] = [re.sub(r\"[^\\w\\s]\", \"\", x).lower() for x in df_text['process_texts']]\n",
        "df_text.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8637a95a",
      "metadata": {
        "id": "8637a95a"
      },
      "outputs": [],
      "source": [
        "# Посмотрим сколько уникальных слов в нашем корпусе текстов\n",
        "\n",
        "results = set()\n",
        "df_text['process_texts'].str.lower().str.split().apply(results.update)\n",
        "print('Количество слов в массиве с обращениями', len(results))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1f3adc23",
      "metadata": {
        "id": "1f3adc23"
      },
      "outputs": [],
      "source": [
        "\n",
        "texts = df_text['process_texts'].values #Извлекаем все тексты обращений\n",
        "\n",
        "classes = list(df_text['sphera'].values) #Извлекаем соответствующие им значения классов\n",
        "\n",
        "maxWordsCount = 60000 #Зададим максимальное количество слов/индексов, учитываемое при составлении векторов для обучения\n",
        "\n",
        "print(df_text['sphera'].unique()) #Выводим все уникальные значения классов\n",
        "\n",
        "nClasses = df_text['sphera'].nunique()+1  #Задаём количество классов, обращаясь к столбцу sphera и оставляя уникальные значения\n",
        "\n",
        "# Прибавляем 1 из-за особенностей использования one-hot encoding с to_categorical()\n",
        "\n",
        "print(nClasses) #Посмотрим на количество классов"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "411577d5",
      "metadata": {
        "id": "411577d5"
      },
      "outputs": [],
      "source": [
        "len(classes)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "254f85a6",
      "metadata": {
        "id": "254f85a6"
      },
      "outputs": [],
      "source": [
        "texts.shape"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Формирование векторов/массивов признаков (feature vectors) для обучения модели\n",
        "Для использования текстовых данных в машинном обучении нам необходимо получить для каждого документа в коллекции числовой вектор признаков фиксированной длины (feature vector)."
      ],
      "metadata": {
        "id": "b8s5cKCpt1Bb"
      },
      "id": "b8s5cKCpt1Bb"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c26d4e78",
      "metadata": {
        "id": "c26d4e78"
      },
      "outputs": [],
      "source": [
        "# Воспользуемся встроенной в Keras функцией класса Tokenizer для разбиения текста и\n",
        "# превращения в матрицу числовых значений\n",
        "\n",
        "tokenizer = Tokenizer(num_words=maxWordsCount, filters='!\"#$%&()*+,-./:;<=>?@[\\\\]^_`{|}~\\t\\n',\n",
        "                      lower=True, split=' ', oov_token='unknown', char_level=False)\n",
        "\n",
        "tokenizer.fit_on_texts(texts) #\"Скармливаем\" наши тексты, т.е. даём в обработку методу, который соберет словарь частотности\n",
        "\n",
        "# Формируем матрицу индексов по принципу Bag of Words\n",
        "xAll = tokenizer.texts_to_matrix(texts) #Каждое слово из текста нашло свой индекс в векторе длиной maxWordsCount\n",
        "# и отметилось в нем единичкой\n",
        "print(xAll.shape)  #Посмотрим на форму текстов\n",
        "print(xAll[0, :20])#И отдельно на фрагмент начала первого вектора"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "397aca11",
      "metadata": {
        "id": "397aca11"
      },
      "outputs": [],
      "source": [
        "# Посмотрим, что такое матрица индексов и сравним с исходным текстом\n",
        "# Исходный текст\n",
        "texts[0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "66ad6fa8",
      "metadata": {
        "id": "66ad6fa8"
      },
      "outputs": [],
      "source": [
        "# Получим список слов, входящих в наш токенизатор (при наличии установленного maxWordsCount)\n",
        "list_columns = ['0']\n",
        "for key_word in list(tokenizer.word_index.keys())[:maxWordsCount-1]:\n",
        "    list_columns.append(key_word)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3d61fc4d",
      "metadata": {
        "id": "3d61fc4d"
      },
      "outputs": [],
      "source": [
        "df_1 = pd.DataFrame([xAll[0]]) #берем матричный вид первого обращения\n",
        "df_1.columns = list_columns\n",
        "df_1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b4064571",
      "metadata": {
        "id": "b4064571"
      },
      "outputs": [],
      "source": [
        "# перейдем от индексов обратно к словами\n",
        "df_1[df_1>0].dropna(axis = 1).columns"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "46379b3b",
      "metadata": {
        "id": "46379b3b"
      },
      "source": [
        "## Кодирование категориальных целевых перемменных (Labelling)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "503a0909",
      "metadata": {
        "id": "503a0909"
      },
      "outputs": [],
      "source": [
        "#Преобразовываем категории в вектор целевой переменной\n",
        "encoder = LabelEncoder() # Вызываем метод кодирования тестовых лейблов из библиотеки sklearn\n",
        "encoder.fit(classes) # Подгружаем в него категории из нашей базы\n",
        "classesEncoded = encoder.transform(classes) # Кодируем категории\n",
        "print(\"Все классы: \", encoder.classes_)\n",
        "print(\"Длина вектора: \", classesEncoded.shape)\n",
        "print(\"Начало вектора: \",classesEncoded[:20])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a8df87ba",
      "metadata": {
        "id": "a8df87ba"
      },
      "outputs": [],
      "source": [
        "yAll = utils.to_categorical(classesEncoded, nClasses) # И выводим каждый лейбл в виде вектора длиной 22,\n",
        "# с 1кой в позиции соответствующего класса и нулями (one-hot encoding)\n",
        "print(\"Форма полученной матрицы: \", yAll.shape)\n",
        "print(\"Отдельная вектор-строка матрицы, указывающая на класс \" + encoder.classes_[classesEncoded[0]] + \": \", yAll[0]) # И отдельный вектор - строку матрицы, указывающую на класс 'Дороги и транспорт'"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b5bcf078",
      "metadata": {
        "id": "b5bcf078"
      },
      "source": [
        "## Построение простейшей нейронной сети для классификации в Tensorflow.Keras"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cc6cd593",
      "metadata": {
        "id": "cc6cd593"
      },
      "outputs": [],
      "source": [
        "# разбиваем все данные на обучающую и тестовую выборки с помощью метода train_test_split из библиотеки sklearn\n",
        "xTrain, xVal, yTrain, yVal = train_test_split(xAll, yAll, test_size=0.2, shuffle = True)\n",
        "print(xTrain.shape) #посмотрим на форму текстов из обучающей выборки\n",
        "print(yTrain.shape) #и на форму соответсвующих им классов"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "hZu87XZhlKAq"
      },
      "id": "hZu87XZhlKAq",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f2b92c66",
      "metadata": {
        "id": "f2b92c66"
      },
      "outputs": [],
      "source": [
        "#Создаём полносвязную сеть\n",
        "model01 = Sequential()\n",
        "#Входной полносвязный слой\n",
        "model01.add(Dense(100, input_dim=maxWordsCount,\n",
        "                  activation=\"relu\"))\n",
        "#Слой регуляризации Dropout\n",
        "model01.add(Dropout(0.4))\n",
        "#Второй полносвязный слой\n",
        "model01.add(Dense(100, activation='relu'))\n",
        "#Слой регуляризации Dropout\n",
        "model01.add(Dropout(0.4))\n",
        "#Третий полносвязный слой\n",
        "model01.add(Dense(100, activation='relu'))\n",
        "#Слой регуляризации Dropout\n",
        "model01.add(Dropout(0.4))\n",
        "#Выходной полносвязный слой\n",
        "model01.add(Dense(nClasses, activation='softmax'))\n",
        "\n",
        "\n",
        "model01.compile(optimizer='adam',\n",
        "              loss='categorical_crossentropy',\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "#Обучаем сеть на выборке\n",
        "history = model01.fit(xTrain,\n",
        "                    yTrain,\n",
        "                    epochs=20,\n",
        "                    batch_size=128,\n",
        "                    validation_data=(xVal, yVal))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Оценка качества обучения  "
      ],
      "metadata": {
        "id": "4ug249-xKvYn"
      },
      "id": "4ug249-xKvYn"
    },
    {
      "cell_type": "code",
      "source": [
        "plt.plot(history.history['accuracy'],\n",
        "         label='Доля верных ответов на обучающем наборе')\n",
        "plt.plot(history.history['val_accuracy'],\n",
        "         label='Доля верных ответов на проверочном наборе')\n",
        "plt.xlabel('Эпоха обучения')\n",
        "plt.ylabel('Доля верных ответов')\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "h5pR-DzNk-ct"
      },
      "id": "h5pR-DzNk-ct",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "dba24972",
      "metadata": {
        "id": "dba24972"
      },
      "outputs": [],
      "source": [
        "plt.figure(figsize=(14,7))\n",
        "plt.plot(history.history['loss'],\n",
        "         label='Ошибка на обучающем наборе')\n",
        "plt.plot(history.history['val_loss'],\n",
        "         label='Ошибка на проверочном наборе')\n",
        "plt.xlabel('Эпоха обучения')\n",
        "plt.ylabel('Ошибка')\n",
        "plt.legend()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "58ede02f",
      "metadata": {
        "id": "58ede02f"
      },
      "outputs": [],
      "source": [
        "model01.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0d54c3fc",
      "metadata": {
        "id": "0d54c3fc"
      },
      "outputs": [],
      "source": [
        "plot_model(model01, to_file='model_1.png')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "fa1c09ab",
      "metadata": {
        "id": "fa1c09ab"
      },
      "source": [
        "# Формирование прогноза (Inference)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "163a7694",
      "metadata": {
        "id": "163a7694"
      },
      "outputs": [],
      "source": [
        "currPred = model01.predict(xTrain[[0]])\n",
        "#Определяем номер распознанного класса для каждохо блока слов длины xLen\n",
        "currOut = np.argmax(currPred, axis=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c9131890",
      "metadata": {
        "id": "c9131890"
      },
      "outputs": [],
      "source": [
        "currOut"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "77a33e92",
      "metadata": {
        "id": "77a33e92"
      },
      "outputs": [],
      "source": [
        "encoder.inverse_transform(currOut)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Дополнительные задания\n",
        "\n",
        "1.   Удалите стоп-слова из обращений\n",
        "2.   Преобразуйте все слова в обращениях к нормальной форме\n",
        "3.   Как при этом изменились размеры матрицы из векторов признаков?\n",
        "4.   Как это повлияло на результаты обучения нейросети?\n",
        "5.   Используйте tf.keras.layers.TextVectorization вместо Tokenizer\n",
        "\n"
      ],
      "metadata": {
        "id": "DDYXOkJMZn_f"
      },
      "id": "DDYXOkJMZn_f"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "89c1089d",
      "metadata": {
        "id": "89c1089d"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.7"
    },
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 5
}